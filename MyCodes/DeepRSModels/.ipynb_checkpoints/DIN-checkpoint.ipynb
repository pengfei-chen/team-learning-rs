{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 动机\n",
    "\n",
    "Deep Interest Network(DIIN)是2018年阿里巴巴提出来的模型， 该模型基于业务的观察，从实际应用的角度进行改进，相比于之前很多“学术风”的深度模型， 该模型更加具有**业务气息**。该模型的应用场景是阿里巴巴的电商广告推荐业务， 这样的场景下一般**会有大量的用户历史行为信息**， 这个其实是很关键的，因为DIN模型的**创新点**或者**解决的问题**就是使用了**注意力机制**来对用户的兴趣动态模拟， 而这个模拟过程存在的**前提就是用户之前有大量的历史行为了**，这样我们在预测某个商品广告用户是否点击的时候，就可以参考他之前购买过或者查看过的商品，这样就能猜测出用户的大致兴趣来，这样我们的推荐才能做的更加到位，所以这个模型的使用场景是**非常注重用户的历史行为特征（历史购买过的商品或者类别信息）**，也希望通过这一点，能够和前面的一些深度学习模型对比一下。\n",
    "\n",
    "在个性化的电商广告推荐业务场景中，也正式由于用户留下了大量的历史**交互行为**，才更加看出了之前的深度学习模型(作者统称Embeding&MLP模型)的不足之处。如果学习了前面的各种深度学习模型，就会发现Embeding&MLP模型对于这种推荐任务一般有着差不多的固定处理套路，就是**大量稀疏特征先经过embedding层， 转成低维稠密的，然后进行拼接，最后喂入到多层神经网络中去**。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些模型在这种个性化广告点击预测任务中存在的问题就是**无法表达用户广泛的兴趣**，因为这些模型在得到各个特征的embedding之后，就蛮力拼接了，然后就各种交叉等。这时候**根本没有考虑之前用户历史行为商品具体是什么，究竟用户历史行为中的哪个会对当前的点击预测带来积极的作用**。 而实际上，对于用户点不点击当前的商品广告，很大程度上是依赖于他的历史行为的，王喆老师举了个例子\n",
    "\n",
    ">假设广告中的商品是键盘， 如果用户历史点击的商品中有化妆品， 包包，衣服， 洗面奶等商品， 那么大概率上该用户可能是对键盘不感兴趣的， 而如果用户历史行为中的商品有鼠标， 电脑，iPad，手机等， 那么大概率该用户对键盘是感兴趣的， 而如果用户历史商品中有鼠标， 化妆品， T-shirt和洗面奶， 鼠标这个商品embedding对预测“键盘”广告的点击率的重要程度应该大于后面的那三个。\n",
    "\n",
    "这里也就是说如果是之前的那些深度学习模型，是没法很好的去表达出用户这广泛多样的兴趣的，如果想表达的准确些， 那么就得**加大隐向量的维度**，**让每个特征的信息更加丰富**， 那这样带来的问题就是计算量上去了，毕竟真实情景尤其是电商广告推荐的场景，特征维度的规模是非常大的。 并且根据上面的例子， 也**并不是用户所有的历史行为特征都会对某个商品广告点击预测起到作用**。所以对于当前某个商品广告的点击预测任务，没必要考虑之前所有的用户历史行为。 \n",
    "\n",
    "这样， DIN的动机就出来了，在业务的角度，我们应该**自适应的去捕捉用户的兴趣变化**，这样才能较为准确的实施广告推荐；而放到模型的角度， 我们应该**考虑到用户的历史行为商品与当前商品广告的一个关联性**，如果用户历史商品中很多与当前商品关联，那么说明该商品可能符合用户的品味，就把该广告推荐给他。而一谈到关联性的话， 我们就容易想到“注意力”的思想了， 所以为了更好的从用户的历史行为中学习到与当前商品广告的关联性，学习到用户的兴趣变化， 作者把注意力引入到了模型，设计了一个\"local activation unit\"结构，**利用候选商品和历史问题商品之间的相关性计算出权重，这个就代表了对于当前商品广告的预测**，用户历史行为的各个商品的重要程度大小， 而加入了注意力权重的深度学习网络，就是这次的主角DIN， 下面具体来看下该模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "注意力机制，我目前在做的 驾驶舱项目，岂不是非常类似哈哈哈\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 基线模型\n",
    "\n",
    "这里的base 模型，就是上面提到过的Embedding&MLP的形式， 这个之所以要介绍，就是因为DIN网络的基准也是他，只不过在这个的基础上添加了一个新结构(注意力网络)来学习当前候选广告与用户历史行为特征的相关性，从而动态捕捉用户的兴趣。\n",
    "\n",
    "基准模型的结构相对比较简单，我们前面也一直用这个基准， 分为三大模块：Embedding layer，Pooling & Concat layer和MLP， 结构如下:\n",
    "\n",
    "<img src=\"http://ryluo.oss-cn-chengdu.aliyuncs.com/图片3.png\" style=\"zoom:67%;\" />\n",
    "\n",
    "\n",
    "前面的大部分深度模型结构也是遵循着这个范式套路， 简介一下各个模块。\n",
    "\n",
    "1. **Embedding layer**：这个层的作用是把高维稀疏的输入转成低维稠密向量， 每个离散特征下面都会对应着一个embedding词典， 维度是$D\\times K$， 这里的$D$表示的是隐向量的维度， 而$K$表示的是当前离散特征的唯一取值个数,  这里为了好理解，这里举个例子说明，就比如上面的weekday特征：\n",
    "\n",
    ">  假设某个用户的weekday特征就是周五，化成one-hot编码的时候，就是[0,0,0,0,1,0,0]表示，这里如果再假设隐向量维度是D， 那么这个特征对应的embedding词典是一个$D\\times7$的一个矩阵(每一列代表一个embedding，7列正好7个embedding向量，对应周一到周日)，那么该用户这个one-hot向量经过embedding层之后会得到一个$D\\times1$的向量，也就是周五对应的那个embedding，怎么算的，其实就是$embedding矩阵* [0,0,0,0,1,0,0]^T$ 。其实也就是直接把embedding矩阵中one-hot向量为1的那个位置的embedding向量拿出来。 这样就得到了稀疏特征的稠密向量了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他离散特征也是同理，只不过上面那个multi-hot编码的那个，会得到一个embedding向量的列表，因为他开始的那个multi-hot向量不止有一个是1，这样乘以embedding矩阵，就会得到一个列表了。通过这个层，上面的输入特征都可以拿到相应的稠密embedding向量了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **pooling layer and Concat layer**： pooling层的作用是将用户的历史行为embedding这个最终变成一个**定长**的向量，因为每个用户历史购买的商品数是不一样的， 也就是每个用户multi-hot中1的个数不一致，这样经过embedding层，得到的用户历史行为embedding的**个数不一样多**，也就是上面的embedding列表$t_i$不一样长， 那么这样的话，每个用户的历史行为特征拼起来就不一样长了。 而后面如果加全连接网络的话，我们知道，他需要定长的特征输入。 所以往往用一个pooling layer先把用户历史行为embedding变成固定长度(统一长度)，所以有了这个公式：\n",
    "\n",
    "$$\n",
    "  e_i=pooling(e_{i1}, e_{i2}, ...e_{ik})\n",
    "$$\n",
    "\n",
    "  这里的$e_{ij}$是用户历史行为的那些embedding。$e_i$就变成了定长的向量， 这里的$i$表示第$i$个历史特征组(是历史行为，比如历史的商品id，历史的商品类别id等)， 这里的$k$表示对应历史特种组里面用户购买过的商品数量，也就是历史embedding的数量，看上面图里面的user behaviors系列，就是那个过程了。 Concat layer层的作用就是拼接了，就是把这所有的特征embedding向量，如果再有连续特征的话也算上，从特征维度拼接整合，作为MLP的输入。\n",
    "\n",
    "3. **MLP**：这个就是普通的全连接，用了学习特征之间的各种交互。\n",
    "\n",
    "4. **Loss**: 由于这里是点击率预测任务， 二分类的问题，所以这里的损失函数用的负的log对数似然："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L=-\\frac{1}{N} \\sum_{(\\boldsymbol{x}, y) \\in \\mathcal{S}}(y \\log p(\\boldsymbol{x})+(1-y) \\log (1-p(\\boldsymbol{x})))L=-\\frac{1}{N} \\sum_{(\\boldsymbol{x}, y) \\in \\mathcal{S}}(y \\log p(\\boldsymbol{x})+(1-y) \\log (1-p(\\boldsymbol{x})))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是base 模型的全貌， 这里应该能看出这种模型的问题， 通过上面的图也能看出来， **用户的历史行为特征**和**当前的候选广告特征**在全都拼起来给神经网络之前，是一点交互的过程都没有， 而拼起来之后给神经网络，虽然是有了交互了，但是原来的一些信息，比如，**每个历史商品的信息会丢失了一部分**，因为这个与当前候选广告商品交互的是池化后的历史特征embedding， 这个embedding是综合了所有的历史商品信息， 这个通过我们前面的分析，对于预测当前广告点击率，并不是所有历史商品都有用，综合所有的商品信息反而会增加一些噪声性的信息，可以联想上面举得那个键盘鼠标的例子，如果加上了各种洗面奶，衣服啥的反而会起到反作用。其次就是这样综合起来，**已经没法再看出到底用户历史行为中的哪个商品与当前商品比较相关**，也就是丢失了历史行为中各个商品对当前预测的重要性程度。最后一点就是如果所有用户浏览过的历史行为商品，最后都通过embedding和pooling转换成了固定长度的embedding，这样会限制模型学习用户的多样化兴趣。\n",
    "\n",
    "那么改进这个问题的思路有哪些呢？  第一个就是加大embedding的维度，增加之前各个商品的表达能力，这样即使综合起来，embedding的表达能力也会加强， 能够蕴涵用户的兴趣信息，但是这个在大规模的真实推荐场景计算量超级大，不可取。 另外一个思路就是**在当前候选广告和用户的历史行为之间引入注意力的机制**，这样在预测当前广告是否点击的时候，**让模型更关注于与当前广告相关的那些用户历史产品**，也就是说**与当前商品更加相关的历史行为更能促进用户的点击行为**。 作者这里又举了之前的一个例子：\n",
    "\n",
    "> 想象一下，当一个年轻母亲访问电子商务网站时，她发现展示的新手袋很可爱，就点击它。让我们来分析一下点击行为的驱动力。<br><br>展示的广告通过软搜索这位年轻母亲的历史行为，发现她最近曾浏览过类似的商品，如大手提袋和皮包，从而击中了她的相关兴趣\n",
    "\n",
    "\n",
    "第二个思路就是DIN的改进之处了。DIN通过给定一个候选广告，然后**去注意与该广告相关的局部兴趣的表示来模拟此过程**。 DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给的广告）。 该表示向量随不同广告而变化。下面看一下DIN模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 DIN模型架构\n",
    "\n",
    "上面分析完了base模型的不足和改进思路之后，DIN模型的结构就呼之欲出了，首先，它依然是采用了基模型的结构，只不过是在这个的基础上**加了一个注意力机制来学习用户兴趣与当前候选广告间的关联程度**， 用论文里面的话是，引入了一个新的`local activation unit`， 这个东西用在了用户历史行为特征上面， **能够根据用户历史行为特征和当前广告的相关性给用户历史行为特征embedding进行加权**。我们先看一下它的结构，然后看一下这个加权公式。\n",
    "\n",
    "<img src=\"http://ryluo.oss-cn-chengdu.aliyuncs.com/图片4.png\" style=\"zoom: 67%;\" />\n",
    "\n",
    "**这里改进的地方已经框出来了，这里会发现相比于base model， 这里加了一个local activation unit， 这里面是一个前馈神经网络，输入是用户历史行为商品和当前的候选商品， 输出是它俩之间的相关性， 这个相关性相当于每个历史商品的权重，把这个权重与原来的历史行为embedding相乘求和就得到了用户的兴趣表示$\\boldsymbol{v}_{U}(A)$, 这个东西的计算公式如下：**\n",
    "$$\n",
    "\\boldsymbol{v}_{U}(A)=f\\left(\\boldsymbol{v}_{A}, \\boldsymbol{e}_{1}, \\boldsymbol{e}_{2}, \\ldots, \\boldsymbol{e}_{H}\\right)=\\sum_{j=1}^{H} a\\left(\\boldsymbol{e}_{j}, \\boldsymbol{v}_{A}\\right) \\boldsymbol{e}_{j}=\\sum_{j=1}^{H} \\boldsymbol{w}_{j} \\boldsymbol{e}_{j}\n",
    "$$\n",
    "这里的$\\{\\boldsymbol{v}_{A}, \\boldsymbol{e}_{1}, \\boldsymbol{e}_{2}, \\ldots, \\boldsymbol{e}_{H}\\}$是用户$U$的历史行为特征embedding， $v_{A}$表示的是候选广告$A$的embedding向量， $a(e_j, v_A)=w_j$表示的权重或者历史行为商品与当前广告$A$的相关性程度。$a(\\cdot)$表示的上面那个前馈神经网络，也就是那个所谓的注意力机制， 当然，看图里的话，输入除了历史行为向量和候选广告向量外，还加了一个它俩的外积操作，作者说这里是有利于模型相关性建模的显性知识。\n",
    "\n",
    "这里有一点需要特别注意，**就是这里的权重加和不是1， 准确的说这里不是权重， 而是直接算的相关性的那种分数作为了权重**，也就是平时的那种scores(softmax之前的那个值)，这个是为了保留用户的兴趣强度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DIN实现\n",
    "\n",
    "下面我们看下DIN的代码复现，这里主要是给大家说一下这个模型的设计逻辑，参考了deepctr的函数API的编程风格， 具体的代码以及示例大家可以去参考后面的GitHub，里面已经给出了详细的注释， 这里主要分析模型的逻辑这块。关于函数API的编程式风格，我们还给出了一份文档， 大家可以先看这个，再看后面的代码部分，会更加舒服些。下面开始：\n",
    "\n",
    "这里主要和大家说一下DIN模型的总体运行逻辑，这样可以让大家从宏观的层面去把握模型的编写过程。该模型所使用的数据集是movielens数据集， 具体介绍可以参考后面的GitHub。 因为上面反复强调了DIN的应用场景，需要基于用户的历史行为数据， **所以在这个数据集中会有用户过去对电影评分的一系列行为**。这在之前的数据集中往往是看不到的。 大家可以导入数据之后自行查看这种行为特征(hist_behavior)。另外还有一点需要说明的是这种历史行为是**序列性质**的特征， 并且**不同的用户这种历史行为特征长度会不一样**， 但是我们的神经网络是要求序列等长的，所以这种情况我们一般会**按照最长的序列进行padding的操作**(不够长的填0)， 而到具体层上进行运算的时候，会用mask掩码的方式标记出这些填充的位置，好保证计算的准确性。 在我们给出的代码中，大家会在AttentionPoolingLayer层的前向传播中看到这种操作。下面开始说编写逻辑：\n",
    "\n",
    "首先， DIN模型的输入特征大致上分为了三类： Dense(连续型), Sparse(离散型),  VarlenSparse(变长离散型)，也就是指的上面的历史行为数据。而不同的类型特征也就决定了后面处理的方式会不同：\n",
    "\n",
    "* Dense型特征：由于是数值型了，这里为每个这样的特征建立Input层接收这种输入， 然后拼接起来先放着，等离散的那边处理好之后，和离散的拼接起来进DNN\n",
    "* Sparse型特征，为离散型特征建立Input层接收输入，然后需要先通过embedding层转成低维稠密向量，然后拼接起来放着，等变长离散那边处理好之后， 一块拼起来进DNN， **但是这里面要注意有个特征的embedding向量还得拿出来用，就是候选商品的embedding向量，这个还得和后面的计算相关性，对历史行为序列加权**。\n",
    "* VarlenSparse型特征：这个一般指的用户的历史行为特征，变长数据， 首先会进行padding操作成等长， 然后建立Input层接收输入，然后通过embedding层**得到各自历史行为的embedding向量**， **拿着这些向量与上面的候选商品embedding向量进入AttentionPoolingLayer去对这些历史行为特征加权合并，最后得到输出**。\n",
    "\n",
    "通过上面的三种处理， 就得到了处理好的连续特征，离散特征和**变长离散特征**， 接下来把这三种特征拼接，进DNN网络，得到最后的输出结果即可。所以有了这个解释， 就可以放DIN模型的代码全貌了，大家可以感受下我上面解释的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "# from deepctr.feature_column import SparseFeat, DenseFeat, VarLenSparseFeat\n",
    "from utils import SparseFeat, DenseFeat, VarLenSparseFeat  #这一章节需要使用本地自带的 utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建输入层\n",
    "# 将输入的数据转换成字典的形式，定义输入层的时候让输入层的name和字典中特征的key一致，就可以使得输入的数据和对应的Input层对应\n",
    "def build_input_layers(feature_columns):\n",
    "    input_layer_dict = {}\n",
    "\n",
    "    for fc in feature_columns:\n",
    "        if isinstance(fc, SparseFeat):\n",
    "            input_layer_dict[fc.name] = Input(shape=(1,), name=fc.name)\n",
    "        elif isinstance(fc, DenseFeat):\n",
    "            input_layer_dict[fc.name] = Input(shape=(fc.dimension, ), name=fc.name)\n",
    "        elif isinstance(fc, VarLenSparseFeat):\n",
    "            input_layer_dict[fc.name] = Input(shape=(fc.maxlen, ), name=fc.name)  # 这一步适应模型，在这里发生了变化\n",
    "    \n",
    "    return input_layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建embedding层\n",
    "def build_embedding_layers(feature_columns, input_layer_dict):\n",
    "    embedding_layer_dict = {}\n",
    "    \n",
    "    # 单一维度的embedding这里没有了吗~\n",
    "    for fc in feature_columns:\n",
    "        if isinstance(fc, SparseFeat):\n",
    "            embedding_layer_dict[fc.name] = Embedding(fc.vocabulary_size, fc.embedding_dim, name='emb_' + fc.name)\n",
    "        elif isinstance(fc, VarLenSparseFeat):\n",
    "            embedding_layer_dict[fc.name] = Embedding(fc.vocabulary_size + 1, fc.embedding_dim, name='emb_' + fc.name, mask_zero=True)\n",
    "\n",
    "    return embedding_layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(feature_columns, input_layer_dict, embedding_layer_dict):\n",
    "    embedding_list = []\n",
    "    for fc in feature_columns:\n",
    "        _input = input_layer_dict[fc]\n",
    "        _embed = embedding_layer_dict[fc]\n",
    "        embed = _embed(_input)\n",
    "        embedding_list.append(embed)\n",
    "\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dice(Layer):\n",
    "    def __init__(self):\n",
    "        super(Dice, self).__init__()\n",
    "        self.bn = BatchNormalization(center=False, scale=False)  # BN正则化\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(shape=(input_shape[-1],), dtype=tf.float32, name='alpha')\n",
    "\n",
    "    def call(self, x):\n",
    "        x_normed = self.bn(x)\n",
    "        x_p = tf.sigmoid(x_normed)\n",
    "        \n",
    "        return self.alpha * (1.0-x_p) * x + x_p * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalActivationUnit(Layer):\n",
    "\n",
    "    def __init__(self, hidden_units=(256, 128, 64), activation='prelu'):\n",
    "        super(LocalActivationUnit, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.linear = Dense(1)\n",
    "        self.dnn = [Dense(unit, activation=PReLU() if activation == 'prelu' else Dice()) for unit in hidden_units]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # query: B x 1 x emb_dim  keys: B x len x emb_dim\n",
    "        query, keys = inputs \n",
    "\n",
    "        # 获取序列长度\n",
    "        keys_len = keys.get_shape()[1]\n",
    "        queries = tf.tile(query, multiples=[1, keys_len, 1])   # (None, len, emb_dim)  \n",
    "\n",
    "        # 将特征进行拼接\n",
    "        att_input = tf.concat([queries, keys, queries - keys, queries * keys], axis=-1) # B x len x 4*emb_dim\n",
    "\n",
    "        # 将原始向量与外积结果拼接后输入到一个dnn中\n",
    "        # 这里没怎么理解。。\n",
    "        att_out = att_input\n",
    "        for fc in self.dnn:\n",
    "            att_out = fc(att_out) # B x len x att_out\n",
    "        att_out = self.linear(att_out) # B x len x 1\n",
    "        att_out = tf.squeeze(att_out, -1) # B x len\n",
    "\n",
    "        return att_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这一块有很多知识点是我不清楚的，有时间再回看\n",
    "\"\"\"\n",
    "class AttentionPoolingLayer(Layer):\n",
    "    def __init__(self, att_hidden_units=(256, 128, 64)):\n",
    "        super(AttentionPoolingLayer, self).__init__()\n",
    "        self.att_hidden_units = att_hidden_units\n",
    "        self.local_att = LocalActivationUnit(self.att_hidden_units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # keys: B x len x emb_dim, queries: B x 1 x emb_dim\n",
    "        queries, keys = inputs \n",
    "\n",
    "        # 获取行为序列embedding的mask矩阵，将Embedding矩阵中的非零元素设置成True，\n",
    "        key_masks = tf.not_equal(keys[:,:,0], 0) # B x len\n",
    "        # key_masks = keys._keras_mask # tf的有些版本不能使用这个属性，2.1是可以的，2.4好像不行\n",
    "        #  我是2.4.。。\n",
    "\n",
    "        # 获取行为序列中每个商品对应的注意力权重\n",
    "        attention_score = self.local_att([queries, keys]) # B x len\n",
    "\n",
    "        # 去除最后一个维度，方便后续理解与计算\n",
    "        # outputs = attention_score\n",
    "        # 创建一个padding的tensor, 目的是为了标记出行为序列embedding中无效的位置\n",
    "        paddings = tf.zeros_like(attention_score) # B x len\n",
    "\n",
    "        # outputs 表示的是padding之后的attention_score\n",
    "        outputs = tf.where(key_masks, attention_score, paddings) # B x len\n",
    "\n",
    "        # 将注意力分数与序列对应位置加权求和，这一步可以在\n",
    "        outputs = tf.expand_dims(outputs, axis=1) # B x 1 x len\n",
    "\n",
    "        # keys : B x len x emb_dim\n",
    "        outputs = tf.matmul(outputs, keys) # B x 1 x dim\n",
    "        outputs = tf.squeeze(outputs, axis=1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN logits\n",
    "def get_dnn_logits(dnn_input, hidden_units=(200, 80), activation='prelu'):\n",
    "    dnns = [Dense(unit, activation=PReLU() if activation == 'prelu' else Dice()) for unit in hidden_units]\n",
    "\n",
    "    dnn_out = dnn_input\n",
    "    for dnn in dnns:\n",
    "        dnn_out = dnn(dnn_out)\n",
    "    \n",
    "    # 获取logits\n",
    "    dnn_logits = Dense(1, activation='sigmoid')(dnn_out)\n",
    "\n",
    "    return dnn_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入层拼接成列表\n",
    "def concat_input_list(input_list):\n",
    "    feature_nums = len(input_list)\n",
    "    if feature_nums > 1:\n",
    "        return Concatenate(axis=1)(input_list)\n",
    "    elif feature_nums == 1:\n",
    "        return input_list[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有的sparse特征embedding拼接\n",
    "def concat_embedding_list(feature_columns, input_layer_dict, embedding_layer_dict, flatten=False):\n",
    "    embedding_list = []\n",
    "    for fc in feature_columns:\n",
    "        _input = input_layer_dict[fc.name] # 获取输入层 \n",
    "        _embed = embedding_layer_dict[fc.name] # B x 1 x dim  获取对应的embedding层\n",
    "        embed = _embed(_input) # B x dim  将input层输入到embedding层中\n",
    "\n",
    "        # 是否需要flatten, 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten，否则不需要\n",
    "        if flatten:\n",
    "            embed = Flatten()(embed)\n",
    "        embedding_list.append(embed)\n",
    "    \n",
    "    return embedding_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "整个逻辑关系，要反复琢磨才能理清\n",
    "\"\"\"\n",
    "def DIN(feature_columns, behavior_feature_list, behavior_seq_feature_list):\n",
    "    # 构建Input层\n",
    "    input_layer_dict = build_input_layers(feature_columns)\n",
    "\n",
    "    # 将Input层转化成列表的形式作为model的输入\n",
    "    input_layers = list(input_layer_dict.values())\n",
    "\n",
    "    # 筛选出特征中的sparse特征和dense特征，方便单独处理\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns))\n",
    "    dense_feature_columns = list(filter(lambda x: isinstance(x, DenseFeat), feature_columns))\n",
    "\n",
    "    # 获取dense\n",
    "    dnn_dense_input = []\n",
    "    for fc in dense_feature_columns:\n",
    "        dnn_dense_input.append(input_layer_dict[fc.name])\n",
    "\n",
    "    # 将所有的dense特征拼接\n",
    "    dnn_dense_input = concat_input_list(dnn_dense_input)\n",
    "\n",
    "    # 构建embedding字典\n",
    "    embedding_layer_dict = build_embedding_layers(feature_columns, input_layer_dict)\n",
    "    \n",
    "    # 因为这里最终需要将embedding拼接后直接输入到全连接层(Dense)中, 所以需要Flatten\n",
    "    dnn_sparse_embed_input = concat_embedding_list(sparse_feature_columns, input_layer_dict, embedding_layer_dict, flatten=True)\n",
    "\n",
    "    # 将所有sparse特征的embedding进行拼接\n",
    "    dnn_sparse_input = concat_input_list(dnn_sparse_embed_input)\n",
    "\n",
    "    # 获取当前的行为特征(movie)的embedding，这里有可能有多个行为产生了行为序列，所以需要使用列表将其放在一起\n",
    "    query_embed_list = embedding_lookup(behavior_feature_list, input_layer_dict, embedding_layer_dict)\n",
    "\n",
    "    # 获取行为序列(movie_id序列, hist_movie_id) 对应的embedding，这里有可能有多个行为产生了行为序列，所以需要使用列表将其放在一起\n",
    "    keys_embed_list = embedding_lookup(behavior_seq_feature_list, input_layer_dict, embedding_layer_dict)\n",
    "    \n",
    "    # 使用注意力机制将历史movie_id序列进行池化\n",
    "    dnn_seq_input_list = []\n",
    "    for i in range(len(keys_embed_list)): \n",
    "        seq_emb = AttentionPoolingLayer()([query_embed_list[i], keys_embed_list[i]])\n",
    "        dnn_seq_input_list.append(seq_emb)\n",
    "\n",
    "    # 将多个行为序列attention poolint 之后的embedding进行拼接\n",
    "    dnn_seq_input = concat_input_list(dnn_seq_input_list)\n",
    "\n",
    "    # 将dense特征，sparse特征，及通过注意力加权的序列特征拼接\n",
    "    dnn_input = Concatenate(axis=1)([dnn_dense_input, dnn_sparse_input, dnn_seq_input])\n",
    "\n",
    "    # 获取最终dnn的logits\n",
    "    dnn_logits = get_dnn_logits(dnn_input, activation='prelu')\n",
    "\n",
    "    model = Model(input_layers, dnn_logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>186,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>186,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>186,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2                                                  3  4    5  6  7\n",
       "0  1  1  1  186,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  1  112  2  1\n",
       "1  1  1  1  186,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  1   38  5  0\n",
       "2  1  1  1  186,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...  1  151  7  0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_data = pd.read_csv(r\"D:\\MyData\\chenpf8\\Recommendation System\\team-learning-rs\\DeepRecommendationModel\\代码\\data\\movie_sample.txt\", sep=\"\\t\", header = None)\n",
    "samples_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'186,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_data[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 3s 42ms/step - loss: 1.4440 - val_loss: 0.4953\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5491 - val_loss: 0.4556\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4593 - val_loss: 0.4645\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4613 - val_loss: 0.4852\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.4418 - val_loss: 0.5574\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 读取数据\n",
    "    samples_data = pd.read_csv(r\"D:\\MyData\\chenpf8\\Recommendation System\\team-learning-rs\\DeepRecommendationModel\\代码\\data\\movie_sample.txt\", sep=\"\\t\", header = None)\n",
    "    samples_data.columns = [\"user_id\", \"gender\", \"age\", \"hist_movie_id\", \"hist_len\", \"movie_id\", \"movie_type_id\", \"label\"]\n",
    "    X = samples_data[[\"user_id\", \"gender\", \"age\", \"hist_movie_id\", \"hist_len\", \"movie_id\", \"movie_type_id\"]]\n",
    "    y = samples_data[\"label\"]\n",
    "\n",
    "    # 转为字典形式\n",
    "    X_train = {\"user_id\": np.array(X[\"user_id\"]), \\\n",
    "            \"gender\": np.array(X[\"gender\"]), \\\n",
    "            \"age\": np.array(X[\"age\"]), \\\n",
    "            \"hist_movie_id\": np.array([[int(i) for i in l.split(',')] for l in X[\"hist_movie_id\"]]), \\\n",
    "            \"hist_len\": np.array(X[\"hist_len\"]), \\\n",
    "            \"movie_id\": np.array(X[\"movie_id\"]), \\\n",
    "            \"movie_type_id\": np.array(X[\"movie_type_id\"])}\n",
    "    y_train = np.array(y)\n",
    "    # 这一步的作用是？\n",
    "    feature_columns = [SparseFeat('user_id', max(samples_data[\"user_id\"])+1, embedding_dim=8), \n",
    "                        SparseFeat('gender', max(samples_data[\"gender\"])+1, embedding_dim=8), \n",
    "                        SparseFeat('age', max(samples_data[\"age\"])+1, embedding_dim=8), \n",
    "                        SparseFeat('movie_id', max(samples_data[\"movie_id\"])+1, embedding_dim=8),\n",
    "                        SparseFeat('movie_type_id', max(samples_data[\"movie_type_id\"])+1, embedding_dim=8),\n",
    "                        DenseFeat('hist_len', 1)]\n",
    "\n",
    "    feature_columns += [VarLenSparseFeat('hist_movie_id', vocabulary_size=max(samples_data[\"movie_id\"])+1, embedding_dim=8, maxlen=50)]\n",
    "\n",
    "    # 行为特征列表，表示的是基础特征\n",
    "    behavior_feature_list = ['movie_id']\n",
    "    # 行为序列特征\n",
    "    behavior_seq_feature_list = ['hist_movie_id']\n",
    "\n",
    "    history = DIN(feature_columns, behavior_feature_list, behavior_seq_feature_list)\n",
    "    \n",
    "    history.compile('adam', 'binary_crossentropy')\n",
    "\n",
    "    history.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VarLenSparseFeat??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于每一块的细节，这里就不解释了，在我们给出的GitHub代码中，我们已经加了非常详细的注释，大家看那个应该很容易看明白， 为了方便大家的阅读，我们这里还给大家画了一个整体的模型架构图，帮助大家更好的了解每一块以及前向传播。（画的图不是很规范，先将就看一下，后面我们会统一在优化一下这个手工图）。\n",
    "\n",
    "<img src=\"http://ryluo.oss-cn-chengdu.aliyuncs.com/图片DIN_aaaa.png\" alt=\"DIN_aaaa\" style=\"zoom: 50%;\" />\n",
    "\n",
    "下面是一个通过keras画的模型结构图，为了更好的显示，数值特征和类别特征都只是选择了一小部分，画图的代码也在github中。\n",
    "\n",
    "![din](http://ryluo.oss-cn-chengdu.aliyuncs.com/图片din.png)\n",
    "\n",
    "## 4. 思考\n",
    "DIN模型在工业上的应用还是比较广泛的， 大家可以自由去**通过查资料**看一下具体实践当中这个模型是怎么用的？ 有什么问题？比如行为序列的制作是否合理， 如果时间间隔比较长的话应不应该分一下段？ 再比如注意力机制那里能不能改成别的计算注意力的方式会好点？(我们也知道注意力机制的方式可不仅DNN这一种)， 再比如注意力权重那里该不该加softmax？  这些其实都是可以值的思考探索的一些问题，根据实际的业务场景，大家也可以总结一些更加有意思的工业上应用该模型的技巧和tricks，欢迎一块讨论和分享。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  回答：\n",
    "这些问题，有点超出我目前的水平啦~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 参考资料\n",
    "\n",
    "* [DIN原论文](https://arxiv.org/pdf/1706.06978.pdf)\n",
    "* [deepctr](https://github.com/shenweichen/DeepCTR)\n",
    "* [AI上推荐 之 AFM与DIN模型（当推荐系统遇上了注意力机制）](https://blog.csdn.net/wuzhongqiang/article/details/109532346)\n",
    "* 王喆 - 《深度学习推荐系统》\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
